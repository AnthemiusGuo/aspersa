#!/usr/bin/perl
# TODO: license and author header

use strict;
use warnings FATAL => 'all';

package tcp_extract;

use English qw(-no_match_vars);
use Time::Local qw(timelocal);
use Getopt::Long qw(GetOptionsFromArray);

# The main() function decides what work to do and delegates it to others.
sub main {

   # Parse and build command-line options
   my $port = 3306;
   my $pipe = 'parse_tcpdump_requests,print_requests';
   GetOptionsFromArray(
      \@ARG,
      'port=i' => \$port,
      'pipe=s' => \$pipe,
   );

   # Build a list of functions to call for the pipeline
   my @pipeline;
   foreach my $func ( split(/,/, $pipe) ) {
      no strict 'refs';
      push @pipeline, &{$func}(
         port  => $port,
         files => \@ARG,
         pipe  => $pipe,
      );
   }

   # The pipeline proceeds as long as the first item returns a true value.  Each
   # iteration ends as soon as an item returns false.
   while ( 1 ) {
      my @input = $pipeline[0]->();
      last unless @input;
      foreach my $item ( @pipeline[1..$#pipeline] ) {
         @input = $item->(@input);
         last unless @input;
      }
   }

   exit 0;
}

# Function to memo-ize and cache repeated calls to timelocal.  Accepts a string,
# outputs an integer.
{
   my ($last, $result);
   # $time = timelocal($sec,$min,$hour,$mday,$mon,$year);
   sub make_ts {
      my ($arg) = @_;
      if ( !$last || $last ne $arg ) {
         my ($year, $mon, $mday, $hour, $min, $sec) = split(/\D/, $arg);
         $result = timelocal($sec, $min, $hour, $mday, $mon - 1, $year);
         $last   = $arg;
      }
      return $result;
   }
}

# Function to generate a pipeline process to parse tcpdump output.  The function
# can go at the start of the pipeline.  It outputs a list of (start-ts, end-ts,
# elapsed-time, IP.port) values.  The result is sorted in *end-ts* order.
sub parse_tcpdump_requests {
   my (%args) = @_;
   my %sessions;
   my $request_id = 0;
   make_file_parser(
      %args,
      callback => sub {
         my ($line) = @_;
         my ( $ts, $us, $src, $dst )
            = $line =~ m#
               (\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})(\.\d{6})
               \sIP\s(\S+)\s>\s(\S+):#x;
         return 1 unless $ts; # This is "next"
         $ts = make_ts($ts) . $us;
         if ( $src =~ m/\.$args{port}$/o ) {
            $sessions{$src} ||= [$ts, $request_id++];
         }
         elsif ($sessions{$dst}) {
            my ($start, $id) = @{$sessions{$dst}};
            my $diff         = $ts - $start;
            delete $sessions{$dst};
            if ( $args{make_packets} ) {
               return 0, ([$start, $id, $src, "req"], [$ts, $id, $src, "res"]);
            }
            else {
               return 0, [$start, $ts, $diff, $src];
            }
         }
      },
   );
}

# Function to generate a pipeline process to parse tcpdump output.  The function
# can go at the start of the pipeline.  It outputs a list of packets (ts,
# request-ID, IP.port, req|res).  Every time a complete request is seen, it
# outputs the start and end packet for that request.  The result is sorted in
# order of the end time of the requests, with the start packet first and the end
# packet last.  This can be fed into the Little's Law computations after sorting
# in <timestamp,ID> order.
#
# This is basically the same code as parse_tcpdump_requests(), so we just use
# that with an "if/else" statement.
sub parse_tcpdump_packets {
   my (%args) = @_;
   parse_tcpdump_requests (
      %args,
      make_packets => 1,
   );
}

# Function to generate a pipeline process (first in pipeline) for parsing the
# sorted output of parse_tcpdump_packets and turning it into throughput -vs-
# concurrency metrics.  Sample input:
#
# 1301957864.031004 250 10.10.18.253.34908 req
# 1301957864.031100 250 10.10.18.253.34908 res
#
# A line is output each time that an input line is seen.  Output fields:
#
# 1 - timestamp
# 2 - client IP address and port number
# 3 - number of requests still pending / in progress
# 4 - total time spent serving requests (increases while #3 is nonzero)
# 5 - weighted time spent serving requests (incremented by #3 * elapsed)
#
# Items 4, 5, and 6 can be used to perform Little's Law computations.
sub parse_littles_metrics {
   my (%args) = @_;
   my $last_ts;         # The last-seen timestamp
   my $in_prg   = 0;    # Field 3: The number of requests currently active
   my $busy     = 0;    # Field 4: The total time spent serving requests
   my $weighted = 0;    # Field 5: The weighted time spent serving requests
   make_file_parser(
      %args,
      callback => sub {
         my ($line) = @_;
         my ($ts, $id, $ip_port, $dir) = split(/\s+/, $line);
         return 1 unless $ts; # This is "next"
         if ( $in_prg > 0 && $last_ts ) {
            my $elapsed = $ts - $last_ts;
            $weighted += $in_prg * $elapsed;
            $busy += $elapsed;
         }
         if ( $dir eq 'req' ) {
            $in_prg++;
         }
         else {
            $in_prg--;
         }
         $last_ts = $ts;
         return (0, [$ts, $ip_port, $in_prg, $busy, $weighted]);
      },
   );
}

# A utility function that is called by other functions to factor some code out.
sub make_file_parser {
   my (%args) = @_;
   my @filenames = @{$args{files}};
   push @filenames, '-' unless @filenames;
   my $fh;
   my $callback = $args{callback};

   return sub {

      if ( !$fh && @filenames ) {
         my $filename = shift(@filenames);
         if ( $filename eq '-' ) {
            $fh = *STDIN;
         }
         else {
            open $fh, "<", $filename or die "Can't open $filename: $OS_ERROR";
         }
      }

      while ( $fh ) {
         while ( my $line = <$fh> ) {
            my ($next, @result) = $callback->($line);
            next if $next;
            return @result if @result;
         }
         close $fh;
         if ( @filenames ) {
            my $filename = shift(@filenames);
            open $fh, "<", $filename or die "Can't open $filename: $OS_ERROR";
         }
         else {
            last;
         }
      }
      return ();
   }
}

# Function to generate a pipeline process for printing the output of the
# parser functions.
sub print_requests {
   my (%args) = @_;
   if ( $args{pipe} =~ m/parse_tcpdump_packets/ ) {
      return sub {
         local $OUTPUT_FIELD_SEPARATOR = " ";
         foreach my $thing ( @_ ) {
            print @$thing, "\n";
         }
      };
   }
   elsif ( $args{pipe} =~ m/parse_tcpdump_requests/ ) {
      return sub {
         printf "%.6f %.6f %.6f %s\n", @{$_[0]};
      };
   }
   else { # parse_littles_metrics
      return sub {
         printf "%s %s %3d %.6f %.6f\n", @{$_[0]};
      };
   }
}

# Function to generate a pipeline parser for the output of parse_tcpdump after
# it's been sorted.  TODO: we can probably wrap up this filename-handling stuff
# in a currying function... actually, TODO I probably don't even need this code.
# Output: the same kind of list that parse_tcpdump() outputs.
sub parse_requests {
   my (%args) = @_;
   my @filenames = @{$args{files}};
   push @filenames, '-' unless @filenames;
   my $fh;

   return sub {

      if ( !$fh && @filenames ) {
         my $filename = shift(@filenames);
         if ( $filename eq '-' ) {
            $fh = *STDIN;
         }
         else {
            open $fh, "<", $filename or die "Can't open $filename: $OS_ERROR";
         }
      }

      while ( $fh ) {
         while ( my $line = <$fh> ) {
            my ( $start, $ts, $diff, $dst ) = $line =~ m/(\S+)/g;
            next unless $ts;
            return $start, $ts, $diff, $dst;
         }
         close $fh;
         if ( @filenames ) {
            my $filename = shift(@filenames);
            open $fh, "<", $filename or die "Can't open $filename: $OS_ERROR";
         }
         else {
            last;
         }
      }
      return ();
   };
}

# This ensures that the entire program can be loaded as a Perl module and
# tested.
exit(main(@ARGV)) unless caller;
1;

=pod

=head1 NAME

tcp-extract - Extract individual TCP requests from a stream.

=head1 SYNOPSIS

Dump TCP requests and responses to a file, capturing only the packet headers to
avoid dropped packets, and ignoring any packets without a payload (such as
ack-only packets).  Capture port 3306 (MySQL database traffic).

 tcpdump -s 384 -i any -nnq -tttt \
	'tcp port 3306 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)' \
   > /path/to/tcp-file.txt

Extract individual response times, sorted by end time:

 tcp-extract /path/to/tcp-file.txt

=head1 DESCRIPTION

This tool recognizes requests and responses in a TCP stream, and extracts the
"conversations".  You can use it to capture the response times of individual
queries to a database, for example.  It expects the input to be in the following
format, which should result from the sample shown in the SYNOPSIS:

 <date> <time.microseconds> IP <IP.port> > <IP.port>: <junk>

The tool watches for "incoming" packets to the port you specify with the --port
option.  This begins a request.  If multiple inbound packets follow each other,
then the last inbound packet seen determines the time at which the request is
assumed to begin.  This is logical if one assumes that a server must receive the
whole SQL statement before beginning execution, for example.

When the first outbound packet is seen, that concludes the request.  The request
is printed out in the following format:

 <start-time> <end-time> <elapsed-time> <IP.port>

There are several challenges in such processing.  First of all, we may see an
inbound packet but never see a response.  This can happen when the kernel drops
packets, for example.  As a result, we never print a request until we see the
response to it, which means that the output is in response order, not request
order.  This can be useful for some kinds of analysis, but others require
requests in request order (start-time order).

Printing requests in start-time order seems to require potentially complex,
computationally inefficient, and memory-intensive algorithms in the worst case.
For example, the pathological case would be that an inbound packet arrives at
the beginning of the stream, but there is never a corresponding response.  We
would have to queue up the entire input and only emit it after seeing the end of
file.

=cut
